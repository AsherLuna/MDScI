{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dccd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan las librerias y se crean los datos con los símbolos a eliminar\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "_SYMBOLS = set(\";:,.\\\\-\\\"'/()[]¿?¡!{}~<>|«»-—’\\t\\n\\r\")\n",
    "quitar=string.punctuation\n",
    "STOPWORDS_ES = stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe483fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Text, eliminating multiple letters in words and accents\n",
    "def normalize_text(text,\n",
    "                   accents=False,\n",
    "                   max_dup=2):\n",
    "\n",
    "    nfkd_f = unicodedata.normalize('NFKD', text)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    return (unicodedata.normalize('NFKC', \"\".join(n_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d74d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This particular function eliminates the majory of the emojis\n",
    "def deEmojify(x):\n",
    "    regrex_pattern = re.compile(pattern= \"[\"\n",
    "                               u\"\\U00000000\\U00000039\"\n",
    "                               u\"\\U00000100-\\U000E0067\"#u\"\\U00002660-\\U0001F972\"\n",
    "                                   \"]\", flags=re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a01201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover StopWords in spanish\n",
    "def filter_words_es(text, remove_stopwords=True):\n",
    "    wlist = text.split()\n",
    "    o_text = []\n",
    "    # filtrado de palabras\n",
    "    for t in wlist:\n",
    "        if remove_stopwords and t in STOPWORDS_ES:\n",
    "            continue\n",
    "        if t.isnumeric():\n",
    "            continue\n",
    "        o_text.append(t)\n",
    "    return \" \".join(o_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bf05012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function applies all the functions created above\n",
    "def limpieza(twitts):\n",
    "    nuevo=[]\n",
    "    for twitt in twitts:\n",
    "        modificacion=re.sub(r'@\\w+','', twitt) #elimina usuarios\n",
    "        modificacion= re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', modificacion) #elimina URLs\n",
    "        modificacion= re.sub(r'#\\w+', '', modificacion) #elimina Hashtags\n",
    "        modificacion=modificacion.lower() #cambia a minúsculas\n",
    "        modificacion=normalize_text(modificacion) #normaliza el texto\n",
    "        modificacion=filter_words_es(modificacion)\n",
    "        for caracter in quitar:\n",
    "            modificacion=modificacion.replace(caracter,\n",
    "                         \"\")\n",
    "            for caracter in _SYMBOLS:\n",
    "                modificacion=modificacion.replace(caracter,\n",
    "                             \"\")\n",
    "        modificacion=deEmojify(modificacion) #elimina emoticones\n",
    "        nuevo.append(modificacion)\n",
    "    return nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6015ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This command load the file to clean\n",
    "# \"Path\" is the rout were the document is found\n",
    "Path=\"C:/Users/Luna/Documents/GitHub/MDScI/1. Data cleaning Techniques/...\"\n",
    "ruta = pd.read_json(Path,lines=True)\n",
    "docs=limpieza(ruta[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39025ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 18546\n",
      "tokens únicos: 6473\n"
     ]
    }
   ],
   "source": [
    "#Get all the words found in the JSON file and extracts the remaining word and count all the unique words\n",
    "voc=[]\n",
    "for d in docs:\n",
    "    for t in d.split():\n",
    "        voc.append(t.lower())\n",
    "vocab=list(sorted(set(voc)))\n",
    "print(\"tokens:\",len(voc))\n",
    "print(\"tokens únicos:\", len(vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
