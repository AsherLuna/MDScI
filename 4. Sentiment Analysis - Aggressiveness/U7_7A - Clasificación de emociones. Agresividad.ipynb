{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$Procesamiento\\;de\\;información$$\n",
    "\n",
    "# Tarea 7. Detección de emociones\n",
    "## Asher Yoav Luna Osorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis de sentimiento o sentiment analysis consiste en evaluar las emociones, actitudes y opiniones. Las organizaciones utilizan este método para obtener información que les permita comprender la forma en la que los clientes reaccionan respecto a un producto o servicio específico.\n",
    "\n",
    "La herramienta de análisis de sentimiento utiliza tecnologías avanzadas de inteligencia artificial, como procesamiento del lenguaje natural, análisis de texto y ciencia de datos para identificar, extraer y estudiar información subjetiva. De esta forma se realiza la categorización para definir el sentido de la información. Específicamente para este trabajo, textos agresivos o no agresivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio se realiza el entrenamiento de un sistema que nos ayudará a clasificar comentario en Twitter y definir si incitan a la violencia y la discriminación o no. Los textos se procesan para eliminar usuarios, hastags, urls's, signos de puntuación y emoticonse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUACTION = \";:,.\\\\-\\\"'/\"\n",
    "SYMBOLS = \"()[]¿?¡!{}~<>|\"\n",
    "SKIP_SYMBOLS = set(PUNCTUACTION + SYMBOLS)\n",
    "SKIP_SYMBOLS_AND_SPACES = set(PUNCTUACTION + SYMBOLS + '\\t\\n\\r ')\n",
    "\n",
    "def normalize_text(input_str,\n",
    "                   punct=False,\n",
    "                   accents=False,\n",
    "                   max_dup=2):\n",
    "    nfkd_f = unicodedata.normalize('NFKD', input_str)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not punct:\n",
    "            if c in SKIP_SYMBOLS:\n",
    "                continue\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    return \"\".join(n_str)\n",
    "\n",
    "def line_iterator(filename):\n",
    "    if filename.endswith(\".gz\"):\n",
    "        f = gzip.GzipFile(filename)\n",
    "    else:\n",
    "        f = open(filename, encoding='utf8')\n",
    "\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        # Test the type of the line and encode it if neccesary...\n",
    "        if type(line) is bytes:\n",
    "            line = str(line, encoding='utf8')\n",
    "\n",
    "        # If the line is empty, we are done...\n",
    "        if len(line) == 0:\n",
    "            break\n",
    "\n",
    "        line = line.strip()\n",
    "        # If line is empty, jump to next...\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "\n",
    "        yield line\n",
    "\n",
    "    # Close the file...\n",
    "    f.close()\n",
    "\n",
    "def tweet_iterator(filename):\n",
    "    for line in line_iterator(filename):\n",
    "        yield json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(file):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for review in tweet_iterator(file):\n",
    "        text = normalize_text(review['text'])\n",
    "        X.append(text.lower())\n",
    "        Y.append(review['klass'])\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta=\"C:/Users/Luna/Downloads/Maestria/Procesamiento de información/Unidad 6-7/Tarea 7/\"\n",
    "training=\"AggressivenessDetection_train.json\"\n",
    "prediction=\"AggressivenessDetection_predict.json\"\n",
    "X, Y= load_corpus(ruta+training)\n",
    "data, agresive = load_corpus(ruta+prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords.words('spanish'))  \n",
    "X_vec_tf = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(random_state=0, max_iter=3000)\n",
    "svc.fit(X_vec_tf, Y)\n",
    "svc_predict = svc.predict(X_vec_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar predicción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vec_tf1 = vectorizer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set de datos SVM\n",
    "svc_predict_tf = svc.predict(data_vec_tf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificar=[]\n",
    "for j in range(0,25):\n",
    "    D=(data[j],svc_predict_tf[j])\n",
    "    clasificar.append(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se visualiza una muestra de 25 Tweets y su clasificación.<br>\n",
    "<br>\n",
    "$\\quad$ \\* 0=No agresivo<br>\n",
    "$\\quad$ \\* 1=Agresivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_00520 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_00520_row0_col0, #T_00520_row0_col1, #T_00520_row1_col0, #T_00520_row1_col1, #T_00520_row2_col0, #T_00520_row2_col1, #T_00520_row3_col0, #T_00520_row3_col1, #T_00520_row4_col0, #T_00520_row4_col1, #T_00520_row5_col0, #T_00520_row5_col1, #T_00520_row6_col0, #T_00520_row6_col1, #T_00520_row7_col0, #T_00520_row7_col1, #T_00520_row8_col0, #T_00520_row8_col1, #T_00520_row9_col0, #T_00520_row9_col1, #T_00520_row10_col0, #T_00520_row10_col1, #T_00520_row11_col0, #T_00520_row11_col1, #T_00520_row12_col0, #T_00520_row12_col1, #T_00520_row13_col0, #T_00520_row13_col1, #T_00520_row14_col0, #T_00520_row14_col1, #T_00520_row15_col0, #T_00520_row15_col1, #T_00520_row16_col0, #T_00520_row16_col1, #T_00520_row17_col0, #T_00520_row17_col1, #T_00520_row18_col0, #T_00520_row18_col1, #T_00520_row19_col0, #T_00520_row19_col1, #T_00520_row20_col0, #T_00520_row20_col1, #T_00520_row21_col0, #T_00520_row21_col1, #T_00520_row22_col0, #T_00520_row22_col1, #T_00520_row23_col0, #T_00520_row23_col1, #T_00520_row24_col0, #T_00520_row24_col1 {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_00520\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_00520_level0_col0\" class=\"col_heading level0 col0\" >Tweet</th>\n",
       "      <th id=\"T_00520_level0_col1\" class=\"col_heading level0 col1\" >Clasificación</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row0_col0\" class=\"data row0 col0\" >perra estamos en mexico al menos en alguna lengua nativa hubieras tuiteado inche gorda\n",
       "</td>\n",
       "      <td id=\"T_00520_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row1_col0\" class=\"data row1 col0\" >muchas felicidades mi reyna que la pases bien en tu dia de las putas\n",
       "</td>\n",
       "      <td id=\"T_00520_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row2_col0\" class=\"data row2 col0\" >apenas termine mi tarea y me levanto en 4 horas me lleva la verga\n",
       "</td>\n",
       "      <td id=\"T_00520_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row3_col0\" class=\"data row3 col0\" >inconscientemente me hice dependiente de dos personas ahora mira tu si soy el puto amo\n",
       "</td>\n",
       "      <td id=\"T_00520_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row4_col0\" class=\"data row4 col0\" > @usuario @usuario @usuario ese conchadesumadre del yerko que ni siquiera le alcanza pa maricon porque hay que ser hombre antes y a ese le alcanza pa la mitad apenas\n",
       "</td>\n",
       "      <td id=\"T_00520_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row5_col0\" class=\"data row5 col0\" >y si en tus putos 16anos de vida no as sido que se hace\n",
       "</td>\n",
       "      <td id=\"T_00520_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row6_col0\" class=\"data row6 col0\" >apoyemos a nuestras mujeres  no puede seguir habiendo tanta inseguridad tambien #micasaestucasahermana  gordas y feas abstenerse\n",
       "</td>\n",
       "      <td id=\"T_00520_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row7_col0\" class=\"data row7 col0\" >lo unico que quiero es que me dejen en paz con las putas indirectas e insinuaciones\n",
       "</td>\n",
       "      <td id=\"T_00520_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row8_col0\" class=\"data row8 col0\" >un dia mas de sonar que conozco a @usuario y despertar y darme cuenta que pues ni madres solo era otro sueno\n",
       "</td>\n",
       "      <td id=\"T_00520_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row9_col0\" class=\"data row9 col0\" >httpstco4ytvgqgfij y llora el marica por tanto crimen que el mismo propicia\n",
       "</td>\n",
       "      <td id=\"T_00520_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row10_col0\" class=\"data row10 col0\" > @usuario gran parte se destinira para las putas ratas del pais ojala hicieran una fiscalia q cuidara las donaciones y se hicieran publico lo gastado\n",
       "</td>\n",
       "      <td id=\"T_00520_row10_col1\" class=\"data row10 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row11_col0\" class=\"data row11 col0\" >disfruten putos por que ahora que estemos en 2da les vamos a poner una chinga @usuario  @usuario 🤣🤣😂😂\n",
       "</td>\n",
       "      <td id=\"T_00520_row11_col1\" class=\"data row11 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row12_col0\" class=\"data row12 col0\" >la gente ridicula bloquea como si bloqueando eliminando sus heridas o los ardidas se les quitara url\n",
       "</td>\n",
       "      <td id=\"T_00520_row12_col1\" class=\"data row12 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row13_col0\" class=\"data row13 col0\" >si soy serio y callado no es porque sea timido o mamon es que al chile todos me caen de la puta verga\n",
       "</td>\n",
       "      <td id=\"T_00520_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row14_col0\" class=\"data row14 col0\" >clima de la verga trafico de la chingada por todos lados gente caotica estos son los estragos de que @usuario no halla cogido  🌩\n",
       "</td>\n",
       "      <td id=\"T_00520_row14_col1\" class=\"data row14 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row15_col0\" class=\"data row15 col0\" >@usuario @usuario @usuario casualmente aparecera con placas de cd mx con eso salva el puesto no tiene madre\n",
       "</td>\n",
       "      <td id=\"T_00520_row15_col1\" class=\"data row15 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row16_col0\" class=\"data row16 col0\" >los huesos pal perro mi amor😌💅 \n",
       "</td>\n",
       "      <td id=\"T_00520_row16_col1\" class=\"data row16 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row17_col0\" class=\"data row17 col0\" >osecac sos la cara de la verga hijos de la gran puta forros\n",
       "</td>\n",
       "      <td id=\"T_00520_row17_col1\" class=\"data row17 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row18_col0\" class=\"data row18 col0\" >es muy sad como lisa siempre vale verga y a bart todo le sale bien a pesar de ser un bastardo \n",
       "</td>\n",
       "      <td id=\"T_00520_row18_col1\" class=\"data row18 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row19_col0\" class=\"data row19 col0\" >esta puta asiatica le meten una polla gorda para que goce\n",
       "</td>\n",
       "      <td id=\"T_00520_row19_col1\" class=\"data row19 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row20_col0\" class=\"data row20 col0\" >solo espero que hoy no me toque el mismo #traficogt ee todas las putas mananas\n",
       "</td>\n",
       "      <td id=\"T_00520_row20_col1\" class=\"data row20 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row21_col0\" class=\"data row21 col0\" >@usuario @usuario cosas feas del director no se como mommy issues medio mezquinos no se siempre me ha sacado ronchas su obra\n",
       "</td>\n",
       "      <td id=\"T_00520_row21_col1\" class=\"data row21 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row22_col0\" class=\"data row22 col0\" >asi es bien coludidos haciendose de la vista gorda 😡\n",
       "</td>\n",
       "      <td id=\"T_00520_row22_col1\" class=\"data row22 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row23_col0\" class=\"data row23 col0\" >@usuario me dan asco esa clase de cuentas puras ardidas ahi\n",
       "</td>\n",
       "      <td id=\"T_00520_row23_col1\" class=\"data row23 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_00520_row24_col0\" class=\"data row24 col0\" >no senora tomarle fotos encuerado a un joto y despues cogertelo no te hace fotografo\n",
       "</td>\n",
       "      <td id=\"T_00520_row24_col1\" class=\"data row24 col1\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x8fa76548e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frames = pd.DataFrame(clasificar, columns=['Tweet','Clasificación'])\n",
    "data_frames = data_frames.style.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n",
    "data_frames.set_properties(**{'text-align': 'center'}).hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conteo y limpieza de datos\n",
    "\n",
    "Eliminación de URL's, Hashtags, Usuarios, signos de puntuación, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan las librerias y se crean los datos con los símbolos a eliminar\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "\n",
    "_SYMBOLS = set(\";:,.\\\\-\\\"'/()[]¿?¡!{}~<>|«»-—’\\t\\n\\r\")\n",
    "quitar=string.punctuation\n",
    "STOPWORDS_ES = stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = pd.read_json(\"C:/Users/Luna/Downloads/Maestria/Procesamiento de información/Unidad 6-7/Tarea 7/AggressivenessDetection_predict.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizar Text\n",
    "def normalize_text(text,\n",
    "                   accents=False,\n",
    "                   max_dup=2):\n",
    "\n",
    "    nfkd_f = unicodedata.normalize('NFKD', text)\n",
    "    n_str = []\n",
    "    c_prev = ''\n",
    "    cc_prev = 0\n",
    "    for c in nfkd_f:\n",
    "        if not accents and unicodedata.combining(c):\n",
    "            continue\n",
    "        if c_prev == c:\n",
    "            cc_prev += 1\n",
    "            if cc_prev >= max_dup:\n",
    "                continue\n",
    "        else:\n",
    "            cc_prev = 0\n",
    "        n_str.append(c)\n",
    "        c_prev = c\n",
    "    return (unicodedata.normalize('NFKC', \"\".join(n_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar emojis\n",
    "def deEmojify(x):\n",
    "    regrex_pattern = re.compile(pattern= \"[\"\n",
    "                               u\"\\U00000000\\U00000039\"\n",
    "                               u\"\\U00000100-\\U000E0067\"#u\"\\U00002660-\\U0001F972\"\n",
    "                                   \"]\", flags=re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover StopWords en español\n",
    "def filter_words_es(text, remove_stopwords=True):\n",
    "    wlist = text.split()\n",
    "    o_text = []\n",
    "    # filtrado de palabras\n",
    "    for t in wlist:\n",
    "        if remove_stopwords and t in STOPWORDS_ES:\n",
    "            continue\n",
    "        if t.isnumeric():\n",
    "            continue\n",
    "        o_text.append(t)\n",
    "    return \" \".join(o_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica los filtros ya inicializados\n",
    "def limpieza(twitts):\n",
    "    nuevo=[]\n",
    "    for twitt in twitts:\n",
    "        modificacion=re.sub(r'@\\w+','', twitt) #elimina usuarios\n",
    "        modificacion= re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', modificacion) #elimina URLs\n",
    "        modificacion= re.sub(r'#\\w+', '', modificacion) #elimina Hashtags\n",
    "        modificacion=modificacion.lower() #cambia a minúsculas\n",
    "        modificacion=normalize_text(modificacion) #normaliza el texto\n",
    "        modificacion=filter_words_es(modificacion)\n",
    "        for caracter in quitar:\n",
    "            modificacion=modificacion.replace(caracter,\n",
    "                         \"\")\n",
    "            for caracter in _SYMBOLS:\n",
    "                modificacion=modificacion.replace(caracter,\n",
    "                             \"\")\n",
    "        modificacion=deEmojify(modificacion) #elimina emoticones\n",
    "        nuevo.append(modificacion)\n",
    "    return nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=limpieza(ruta[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[]\n",
    "neg=[]\n",
    "for num in range(0,len(docs)):\n",
    "    if svc_predict_tf[num]==0:\n",
    "        pos.append(docs[num])\n",
    "    else:\n",
    "        neg.append(docs[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar(datos):\n",
    "    conjunto=[]\n",
    "    data_frame=[]\n",
    "    for twit in datos:\n",
    "        for word in twit.split():\n",
    "            conjunto.append(word)\n",
    "            \n",
    "    conteo=Counter(conjunto)\n",
    "    sortByValue={k: v for k, v in sorted((conteo.items()), key= lambda v: v[1], reverse=True)}\n",
    "    for i in range (0,50):\n",
    "        for value in sortByValue:\n",
    "            par=(value,sortByValue[value])\n",
    "            data_frame.append(par)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_neg=clasificar(neg)\n",
    "words_pos=clasificar(pos)\n",
    "matrix=[]\n",
    "for i in range(1,51):\n",
    "    matrix.append(i)\n",
    "col1 = ['Palabra Agresiva', 'Cantidad de repeticiones']\n",
    "col2 = ['Palabra No agresiva','Cantidad de repeticiones']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_ag = pd.DataFrame((words_neg[:50]), columns=col1, index=matrix)\n",
    "df_nag = pd.DataFrame((words_pos[:50]), columns=col2, index=matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabla de Palabras agresivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra Agresiva</th>\n",
       "      <th>Cantidad de repeticiones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>putos</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>si</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>madre</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pinche</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>verga</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>putas</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hdp</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>maricon</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>puta</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mierda</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mas</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hijos</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gorda</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>joto</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bien</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pinches</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>puto</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pendejo</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ver</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>q</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>va</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ser</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hijo</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>jajajaja</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>pendejos</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>voy</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>marica</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>solo</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>feas</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>no</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>mamar</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>jajaja</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gente</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tan</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mundial</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ojala</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>chinguen</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>van</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>luego</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>asi</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>chingar</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>mal</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>mamen</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>lameculos</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>mejor</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>amigos</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>toda</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>perra</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>nadie</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>quieren</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Palabra Agresiva  Cantidad de repeticiones\n",
       "1             putos                       146\n",
       "2                si                        86\n",
       "3             madre                        84\n",
       "4            pinche                        73\n",
       "5             verga                        61\n",
       "6             putas                        54\n",
       "7               hdp                        45\n",
       "8           maricon                        41\n",
       "9              puta                        40\n",
       "10           mierda                        33\n",
       "11              mas                        32\n",
       "12            hijos                        30\n",
       "13            gorda                        29\n",
       "14             joto                        29\n",
       "15             bien                        29\n",
       "16          pinches                        28\n",
       "17             puto                        27\n",
       "18          pendejo                        27\n",
       "19              ver                        22\n",
       "20                q                        22\n",
       "21               va                        19\n",
       "22              ser                        18\n",
       "23             hijo                        18\n",
       "24         jajajaja                        17\n",
       "25         pendejos                        16\n",
       "26              voy                        16\n",
       "27           marica                        16\n",
       "28             solo                        16\n",
       "29             feas                        15\n",
       "30               no                        14\n",
       "31            mamar                        13\n",
       "32           jajaja                        13\n",
       "33            gente                        13\n",
       "34              tan                        13\n",
       "35          mundial                        13\n",
       "36            ojala                        13\n",
       "37         chinguen                        12\n",
       "38              van                        12\n",
       "39            luego                        12\n",
       "40              asi                        12\n",
       "41          chingar                        12\n",
       "42              mal                        11\n",
       "43            mamen                        11\n",
       "44        lameculos                        11\n",
       "45            mejor                        11\n",
       "46           amigos                        11\n",
       "47             toda                        11\n",
       "48            perra                        10\n",
       "49            nadie                        10\n",
       "50          quieren                        10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabla de Palabras agresivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra No agresiva</th>\n",
       "      <th>Cantidad de repeticiones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>verga</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>si</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>madre</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loca</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>putas</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mas</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gorda</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>putos</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bien</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>solo</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>feas</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>asi</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ser</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ver</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>puto</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quiero</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vale</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hoy</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dia</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>siempre</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cosas</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ahora</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vida</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>puta</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mamar</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cagado</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>q</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tan</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hace</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>url</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>mejor</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>voy</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gente</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>marica</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>jajajaja</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>va</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>alguien</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>vez</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tontas</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pues</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>cabron</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>luchona</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>jajaja</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>no</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>joto</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>mama</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>hacer</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mamando</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>que</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>estan</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Palabra No agresiva  Cantidad de repeticiones\n",
       "1                verga                       283\n",
       "2                   si                       208\n",
       "3                madre                       201\n",
       "4                 loca                       170\n",
       "5                putas                       161\n",
       "6                  mas                       144\n",
       "7                gorda                       122\n",
       "8                putos                       118\n",
       "9                 bien                        94\n",
       "10                solo                        67\n",
       "11                feas                        66\n",
       "12                 asi                        66\n",
       "13                 ser                        53\n",
       "14                 ver                        51\n",
       "15                puto                        50\n",
       "16              quiero                        49\n",
       "17                vale                        48\n",
       "18                 hoy                        47\n",
       "19                 dia                        45\n",
       "20             siempre                        45\n",
       "21               cosas                        45\n",
       "22               ahora                        43\n",
       "23                vida                        40\n",
       "24                puta                        40\n",
       "25               mamar                        40\n",
       "26              cagado                        40\n",
       "27                   q                        39\n",
       "28                 tan                        39\n",
       "29                hace                        38\n",
       "30                 url                        37\n",
       "31               mejor                        37\n",
       "32                 voy                        37\n",
       "33               gente                        36\n",
       "34              marica                        35\n",
       "35            jajajaja                        35\n",
       "36                  va                        35\n",
       "37             alguien                        34\n",
       "38                 vez                        34\n",
       "39              tontas                        34\n",
       "40                pues                        33\n",
       "41              cabron                        33\n",
       "42             luchona                        32\n",
       "43              jajaja                        32\n",
       "44                  no                        30\n",
       "45                joto                        29\n",
       "46                mama                        29\n",
       "47               hacer                        29\n",
       "48             mamando                        28\n",
       "49                 que                        28\n",
       "50               estan                        27"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la clasificación realizada y las palabras que presentan mayor frecuancias en cada una de las tablas.\n",
    "\n",
    "Observamos que en ambas tablas encontramos en diferente proporción, palabras similares. Esto debido a que la clasificación es realizada a nivel tweet y no a nivel palabra.\n",
    "\n",
    "A continuación se muestran ejemplos de la clasificación realizada\n",
    "\n",
    "`perra estamos en mexico al menos en alguna lengua nativa hubieras tuiteado inche gorda ---> 1`<br>\n",
    "`lo unico que quiero es que me dejen en paz con las putas indirectas e insinuaciones---> 0`\n",
    "\n",
    "Podemos ver la importancia del contexto en los textos que se analiza, ya que sin este se puede realizar una falsa clasificación y obtener información poco confiable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
